{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox-9FiRUCyBh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wleP0pQMCDYl"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/sensor.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/sensor_high_freq.csv\")"
      ],
      "metadata": {
        "id": "YKDUjYm62PYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv(\"/content/drive/MyDrive/percent_reference.csv\")"
      ],
      "metadata": {
        "id": "ZoB0ajNW2T0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr2N3g1RCDjQ"
      },
      "outputs": [],
      "source": [
        "class ExploratoryDataAnalysis():\n",
        "    def __init__(self,dataframe):\n",
        "        self.dataframe = dataframe.copy()\n",
        "        \n",
        "    def _object_to_numeric(self):\n",
        "        # converting to appropriate data type for the measurement columns\n",
        "        rel_col = [x for x in self.dataframe.columns if x[0] ==\"B\"]\n",
        "        if 'Percent' in self.dataframe.columns:\n",
        "            rel_col.append('Percent')\n",
        "        for col in rel_col:\n",
        "            self.dataframe[col] = pd.to_numeric(self.dataframe[col], errors=\"coerce\")\n",
        "    \n",
        "    def _univariate_plots(self):\n",
        "        num_col = self.dataframe.select_dtypes(exclude='object').columns\n",
        "        for col in num_col:\n",
        "            plt.figure(figsize=(15,5))\n",
        "            sns.histplot(self.dataframe[col])\n",
        "            plt.show()\n",
        "        \n",
        "    def _bivariate_plots(self):\n",
        "        num_col = self.dataframe.select_dtypes(exclude='object').columns\n",
        "        combs = combinations(num_col,2)\n",
        "        for comb in combs:\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.scatterplot(x=comb[0],y=comb[1],data=self.dataframe)\n",
        "            plt.show()\n",
        "    \n",
        "    def _outlier_detection(self):\n",
        "        num_col = self.dataframe.select_dtypes(exclude='object').columns\n",
        "        for col in num_col:\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.boxplot(self.dataframe[col])\n",
        "            plt.show()\n",
        "\n",
        "    def plot(self):\n",
        "      self._object_to_numeric()\n",
        "      self._univariate_plots()\n",
        "      self._bivariate_plots()\n",
        "      self._outlier_detection()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessing():\n",
        "    \n",
        "    def __init__(self, df1,df2,df3):\n",
        "        self.df1 = df1\n",
        "        self.df2 = df2\n",
        "        self.df3 = df3\n",
        "\n",
        "    def clean_data(self):\n",
        "        self.df1 = self.data_validation(self.df1)\n",
        "        self.df1 = self.missing_values(self.df1)\n",
        "        self.df2 = self.data_validation(self.df2)\n",
        "        self.df2 = self.missing_values(self.df2)\n",
        "        self.df3 = self.data_validation(self.df3)\n",
        "        self.df3 = self.missing_values(self.df3)\n",
        "\n",
        "    def data_validation(self, dataframe):\n",
        "        # Checking if the sensor and controller measurement columns are numeric\n",
        "        rel_col = [x for x in dataframe.columns if x[0] ==\"B\"]\n",
        "        if 'Percent' in dataframe.columns:\n",
        "            rel_col.append('Percent')\n",
        "        for col in rel_col:\n",
        "            dataframe[col] = pd.to_numeric(dataframe[col], errors=\"coerce\")\n",
        "        return dataframe\n",
        "  \n",
        "            \n",
        "    def missing_values(self, dataframe):\n",
        "        # Tailored starategy for missing values based on the distribution of the variable\n",
        "        \n",
        "        # columns with a skewed distribution(based on univariate analysis)\n",
        "        skew_col = ['B16','B17','B18','B_11','B_13']\n",
        "        # columns with a single value\n",
        "        single_value_col = ['B4','B5', 'B9', 'B10', 'B14', 'B20', 'B22', 'B23']\n",
        "        \n",
        "        normal_col = [x for x in dataframe.columns if x not in skew_col and x not in single_value_col]\n",
        "        # Impute mean for normal columns, median for skewed and mode for single value columns\n",
        "        num_col = dataframe.select_dtypes(exclude='object').columns\n",
        "        for col in num_col:\n",
        "            if col in skew_col:\n",
        "                dataframe[col].fillna(dataframe[col].median(), inplace = True)\n",
        "            if col in single_value_col:\n",
        "                dataframe[col].fillna(dataframe[col].mode(), inplace = True)\n",
        "            else:\n",
        "                dataframe[col].fillna(dataframe[col].mean(), inplace = True)\n",
        "        # Dropping incorrect rows in target variable column\n",
        "        if 'Good/Bad' in dataframe.columns:\n",
        "            dataframe = dataframe[(dataframe['Good/Bad'] == '0') | (dataframe['Good/Bad'] == '1')]                           \n",
        "        return dataframe\n",
        "    \n",
        "    def merging_dataframes(self):\n",
        "        # Generating a common column for joining the dataframes. This is needed as there are no columns to join with\n",
        "        self.df2['ind'] = 1\n",
        "        self.df3['ind'] = 1\n",
        "        df4 = self.df2.merge(self.df3, on='ind',how='left')\n",
        "        # Obtaining the appropriate join using the Percent constraint. Percent_Min < Percent < Percent_Max\n",
        "        df4 = df4[(df4['Percent'] >= df4['Percent Min']) & (df4['Percent'] <= df4['Percent Max'])]\n",
        "        # joining sensor and controller dataframe using timestamp\n",
        "        df5 = df4.merge(self.df1, on='timestamp', how='inner')\n",
        "        return df5\n",
        "    \n",
        "    def drop_redundant(self,df):\n",
        "        cols = ['Cycle ID', 'Period Code', 'Percent Min','Percent Max','ind','timestamp']\n",
        "        df.drop(cols, axis=1, inplace=True)\n",
        "        return df\n",
        "    \n",
        "    def outlier_removal(self,df, threshold=0.01):\n",
        "        # Choosing columns for outlier removal based on boxplots done in EDA\n",
        "        outlier_cols = ['B_18','B_7','B_8','B_13']   # columns with significant outliers\n",
        "        for col in outlier_cols:\n",
        "            if col in df.columns:\n",
        "                q1 = df[col].quantile(threshold/2)\n",
        "                q2 = df[col].quantile(1-(threshold)/2)\n",
        "                df = df[(df[col]<= q2) & (df[col]>=q1)]\n",
        "        return df\n",
        "\n",
        "    def run(self):\n",
        "      self.clean_data()\n",
        "      merged_df = self.merging_dataframes()\n",
        "      abc_df = self.drop_redundant(merged_df)\n",
        "      non_outlier_df = self.outlier_removal(abc_df)\n",
        "      df_train, df_test = self.data_split(non_outlier_df)\n",
        "      df_train_scaled, df_test_scaled = self.feature_scaling(df_train, df_test)\n",
        "      X_train, y_train, X_test, y_test = self.train_test_splitting(df_train_scaled, df_test_scaled, 'Good/Bad')\n",
        "      X_train,y_train,X_test,y_test = self.feature_selection(X_train, y_train, X_test, y_test)\n",
        "      return X_train,y_train,X_test,y_test\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def data_split(self, df, test_size=0.2):\n",
        "        \"\"\" Description: This method splits the dataframe into train and test data respectively\n",
        "            using the sklearn's \"train_test_split\" method.\n",
        "            Parameters: test_size: Percentage of the Dataframe to be taken as a test set\n",
        "            returns: training and testing dataframes respectively.\n",
        "        \"\"\"\n",
        "        df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=42)\n",
        "        return df_train, df_test\n",
        "\n",
        "\n",
        "    def feature_scaling(self, df_train,df_test):\n",
        "        scaler = StandardScaler()\n",
        "        num_col = df_train.select_dtypes(exclude= 'object').columns\n",
        "        cat_col = [x for x in df_train.columns if x not in num_col]\n",
        "        df_train_cat = df_train[cat_col]\n",
        "        df_test_cat = df_test[cat_col]\n",
        "        df_train[num_col] = scaler.fit_transform(df_train[num_col])\n",
        "        df_train_scaled = pd.concat([df_train[num_col], df_train_cat],axis=1)\n",
        "        df_test[num_col] = scaler.fit_transform(df_test[num_col])\n",
        "        df_test_scaled = pd.concat([df_test[num_col], df_test_cat],axis=1)\n",
        "        return df_train_scaled, df_test_scaled\n",
        "        \n",
        "        # return self.train_test_splitting(df_train_scaled, df_test_scaled,'Good/Bad')\n",
        "    \n",
        "\n",
        "    \n",
        "        \n",
        "  \n",
        "    \n",
        "    def train_test_splitting(self, df_train, df_test, column_name):\n",
        "        \"\"\"Description: This method splits the data into dependent and independent variables respectively\n",
        "        i.e., X and y.\n",
        "        Raises an exception if it fails\n",
        "        parameters:\n",
        "        df_train: A pandas dataframe representing the training data set\n",
        "        df_test: A pandas dataframe representing the testing data set\n",
        "        column_name: Target column or feature, which has to be predicted using other features\n",
        "        returns:\n",
        "        independent and dependent features of the both training and testing datasets respectively.\n",
        "        i.e., df_train into X_train, y_train and df_test into X_test, y_test respectively.\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        X_train = df_train.drop(column_name, axis=1)\n",
        "        y_train = df_train[column_name]\n",
        "        X_test = df_test.drop(column_name, axis=1)\n",
        "        y_test = df_test[column_name]\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "            \n",
        "        \n",
        "    def feature_selection(self,X_train,y_train,X_test,y_test):\n",
        "      \n",
        "        single_value_col = ['B_4','B_5', 'B_9', 'B_10', 'B_14', 'B_20', 'B_22', 'B_23']\n",
        "        # remove all variables which have a constant value\n",
        "        for col in single_value_col:\n",
        "            if col in X_train.columns:\n",
        "                X_train = X_train.drop(col,1)\n",
        "                X_test = X_test.drop(col,1)\n",
        "        # check correlation and remove highly correlated predictor variables\n",
        "        corr_matrix = X_train.corr().abs()\n",
        "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "        to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "        X_train = X_train.drop(to_drop,1)\n",
        "        X_test = X_test.drop(to_drop,1)\n",
        "        return X_train,y_train,X_test,y_test\n"
      ],
      "metadata": {
        "id": "kbyl6wWv3BJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = DataPreprocessing(df1, df2, df3).run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVhI2PS3E3P",
        "outputId": "938476db-90cf-4883-9bc1-e876332312fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-5ea3c3df0426>:151: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  X_train = X_train.drop(col,1)\n",
            "<ipython-input-10-5ea3c3df0426>:152: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  X_test = X_test.drop(col,1)\n",
            "<ipython-input-10-5ea3c3df0426>:155: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
            "<ipython-input-10-5ea3c3df0426>:157: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  X_train = X_train.drop(to_drop,1)\n",
            "<ipython-input-10-5ea3c3df0426>:158: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  X_test = X_test.drop(to_drop,1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_v_445mUctK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Models:\n",
        "\n",
        "    def __init__(self, X_train, y_train, X_test, y_test):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def model_evaluation(self, prediction):\n",
        "        score_acc = accuracy_score(self.y_test, prediction)\n",
        "        print(f\"Accuracy of the model is {score_acc}\")\n",
        "        score_f1 = f1_score(self.y_test,prediction, pos_label='1')\n",
        "        print(f\"F-1 score of the model is {score_f1}\")\n",
        "\n",
        "    def model_prediction(self, model, X):\n",
        "        prediction = model.predict(X)\n",
        "        return self.model_evaluation(prediction)\n",
        "\n",
        "    def logistic_regressor(self):\n",
        "        print(\"Evaluating Logistic Regression\")\n",
        "        lm = LogisticRegression()\n",
        "        lm.fit(self.X_train, self.y_train)\n",
        "        X = self.X_test\n",
        "        return self.model_prediction(lm,X)\n",
        "            \n",
        "    def random_forest_classifier(self):\n",
        "        print(\"Evaluating Random Forest\")\n",
        "        RF = RandomForestClassifier()\n",
        "        params = {'n_estimators': [5, 10, 20, 40, 80, 100, 200],\n",
        "                      'max_depth': [2, 5, 10, 20],\n",
        "                      'min_samples_split': [2, 4, 8, 12],\n",
        "                      'oob_score': [True]}\n",
        "\n",
        "            # instantiating RandomizedSearchCV\n",
        "        RCV = RandomizedSearchCV(estimator=RF,\n",
        "                                     param_distributions=params,\n",
        "                                     n_iter=5,\n",
        "                                     scoring='r2',\n",
        "                                     cv=10,\n",
        "                                     verbose=5,\n",
        "                                     random_state=42,\n",
        "                                     n_jobs=-1,\n",
        "                                     return_train_score=True)\n",
        "\n",
        "        # Fitting on the train data\n",
        "        RCV.fit(self.X_train, self.y_train)\n",
        "        RF = RCV.best_estimator_\n",
        "\n",
        "        # fitting on the train data\n",
        "        RF.fit(self.X_train, self.y_train)\n",
        "        X = self.X_test\n",
        "        return self.model_prediction(RF,X)\n",
        "\n",
        "        \n",
        "    def xgboost(self):\n",
        "        print(\"Evaluating XGBoost\")\n",
        "        xgb = XGBClassifier(booster='gbtree', sub_sample=0.8)\n",
        "        params = {'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15],'min_child_weight' : [ 1, 3, 5, 7 ],\n",
        "             'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
        "             'colsample_bytree' : [ 0.3, 0.4, 0.5 , 0.7 ]} \n",
        "        RCV = RandomizedSearchCV(estimator=xgb,\n",
        "                                     param_distributions=params,\n",
        "                                     n_iter=5,\n",
        "                                     scoring='r2',\n",
        "                                     cv=10,\n",
        "                                     verbose=5,\n",
        "                                     random_state=42,\n",
        "                                     n_jobs=-1,\n",
        "                                     return_train_score=True)\n",
        "        RCV.fit(self.X_train, self.y_train)\n",
        "        xgb = RCV.best_estimator_\n",
        "\n",
        "      # fitting on the train data\n",
        "        xgb.fit(self.X_train, self.y_train)\n",
        "        X = self.X_test\n",
        "        return self.model_prediction(xgb,X)\n",
        "\n",
        "    def svc(self):\n",
        "        print(\"Evaluating Support Vector Classifier\")\n",
        "        svc = svm.SVC()\n",
        "        param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}\n",
        "        RCV = RandomizedSearchCV(estimator=svc,\n",
        "                                     param_distributions=param_grid,\n",
        "                                     n_iter=5,\n",
        "                                     scoring='r2',\n",
        "                                     cv=10,\n",
        "                                     verbose=5,\n",
        "                                     random_state=42,\n",
        "                                     n_jobs=-1,\n",
        "                                     return_train_score=True)\n",
        "        RCV.fit(self.X_train, self.y_train)\n",
        "        svc = RCV.best_estimator_\n",
        "        \n",
        "        svc.fit(self.X_train, self.y_train)\n",
        "        X = self.X_test\n",
        "        return self.model_prediction(svc,X)\n",
        "        \n",
        "    \n",
        "    def evaluate(self):\n",
        "      self.logistic_regressor()\n",
        "      self.random_forest_classifier()\n",
        "      self.xgboost()\n",
        "      self.svc()"
      ],
      "metadata": {
        "id": "oMbCVOzS3MJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models( X_train, y_train, X_test, y_test).evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkCWjjQY3UHc",
        "outputId": "276e6f8f-7109-45cd-b499-119aada4e821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Logistic Regression\n",
            "Accuracy of the model is 0.9573630698589701\n",
            "F-1 score of the model is 0.8271276595744682\n",
            "Evaluating Random Forest\n",
            "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
            "Accuracy of the model is 0.959986880944572\n",
            "F-1 score of the model is 0.8342391304347826\n",
            "Evaluating XGBoost\n",
            "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
            "Accuracy of the model is 0.9596589045588717\n",
            "F-1 score of the model is 0.8362183754993342\n",
            "Evaluating Support Vector Classifier\n",
            "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
            "Accuracy of the model is 0.9616267628730731\n",
            "F-1 score of the model is 0.8425302826379543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXrRLOhW4bln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}